
\documentclass[conference]{acmsiggraph}

\usepackage{graphicx}
\graphicspath{{./images/}}

\newcommand{\figuremacroW}[4]{
	\begin{figure}[h] %[htbp]
		\centering
		\includegraphics[width=#4\columnwidth]{#1}
		\caption[#2]{\textbf{#2} - #3}
		\label{fig:#1}
	\end{figure}
}

\newcommand{\figuremacroF}[4]{
	\begin{figure*}[t] % [htbp]
		\centering
		\includegraphics[width=#4\textwidth]{#1}
		\caption[#2]{\textbf{#2} - #3}
		\label{fig:#1}
	\end{figure*}
}

\usepackage{afterpage}
\usepackage{xcolor}
\definecolor{lbcolor}{rgb}{0.98,0.98,0.98}
\usepackage{listings}

\lstset{
	escapeinside={/*@}{@*/},
	language=C++,
	%basicstyle=\small\sffamily,
	%basicstyle=\small\sffamily,	
	basicstyle=\fontsize{8.5}{12}\selectfont,
	%basicstyle=\small\ttfamily,
	%basicstyle=\scriptsize, % \footnotesize,
	%basicstyle=\footnotesize,
	%keywordstyle=\color{blue}\bfseries,
	%basicstyle= \listingsfont,
	numbers=left,
	numbersep=2pt,    
	xleftmargin=2pt,
	%numberstyle=\tiny,
	frame=tb,
	%frame=single,
	columns=fullflexible,
	showstringspaces=false,
	tabsize=4,
	keepspaces=true,
	showtabs=false,
	showspaces=false,
	%showstringspaces=true
	backgroundcolor=\color{lbcolor},
	morekeywords={inline,public,class,private,protected,struct},
	captionpos=t,
	lineskip=-0.4em,
	aboveskip=10pt,
	%belowskip=50pt,
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941}
}

\usepackage{lipsum}

\title{N-body Algorithm Performance analysis}

\author{Sam Serrels\\\ 40082367@napier.ac.uk \\
Edinburgh Napier University\\
Concurrent and Parallel Systems (SET10108)}
\pdfauthor{Sam Serrels}

\keywords{NBody Physics}

\begin{document}

\maketitle

\section{Introduction}

\paragraph{N-body problems}
An N-body simulation is a system of elements, usually particles which interact in some way with every other element in the system. A common example of this would be particles with mass, which calculate the gravitational acceleration applied to them, other examples include plasma physics, fluid dynamics,
and molecular dynamics.
The main complexity of the simulation comes from the operation of each body interacting with every other body, causing a considerable amount of code execution. The Body-Body calculations, such as gravitational acceleration, could be a relatively simple piece of code, but will have to be executed $N^2$ times where $N$ is the number of bodies. Therefore the complexity of an unoptimized simulation is simply $O(n^2)$

\paragraph{Project Scope}
This project investigates the performance of both a CPU and a GPU implemented N-Body Simulation. The CPU implementation was enhanced with the use of multi-threading with OpenMP. Although the initialization code and execution method for each of the two applications is vastly different, the simulation code is the exact same.

\paragraph{OpenMP}
The technology for processing the application in parallel was chosen to be OpenMp, an API that abstracts the creation of threads from the user and therefore allows for easier development and better cross platform portability, assuming that the chosen platform has a complier that supports OpenMP.
This was chosen over creating threads manually, mainly for ease of development reasons, but also because even in a situation that OpenMp is slower than Manual threads, there should still be a noticeable performance increase over the baseline results.

\paragraph{Simulation Implementation}
A simplified version of a gravitational simulation was used, each particle has a mass of 1. A further method was added to attract all bodies back to the center. This simulation was chosen for visual appeal, as only the computational time was being measured.

\section{CPU implementation}
Analysis of the cpu code revealed expected results, the main simulation loop took up almost 100\% of the execution time. Figure \ref{fig:HotCode} visualizes the main loop and highlights the lines of code which take the most time.

\figuremacroW
{HotCode}
{Main Simulation Loop}
{Measure of execution Time}
{0.95}

\figuremacroW
{cpusim}
{Cpu Simulation}
{8096 Particles}
{0.95}

\section{CPU Results}
Different versions of the program were run using a system containing an Intel i7-4790K cpu at 4GHz. Only the time taken to run a step(or "Tick")of the simualtion was measured, time taken to render the particles was not measured. Each result is a average of 1000 ticks where possible, with high particle counts the sample size was gradually reduced to 10 samples.

\subsection{System utilization}
Shown in Figure \ref{fig:omp4_65536_util} and Figure \ref{fig:omp8_65536_util}.
As the mathematical workload of the simulation is straightforward, the cpu utilization was an expected uniform value, corresponding to the amount of threads. With 8 threads, cpu utilization was between 95\% and 100\%. With anything less, the results were almost exactly equal to the ratio of threads to maximum logical cores (4 threads: 50\%, 1 thread: 12.5\%, 2 threads: 25\% ).

\subsection{Core occupancy}
Shown in Figure \ref{fig:omp1_4096_occu}, Figure \ref{fig:omp4_4096_occu}, and Figure \ref{fig:omp8_4096_occu}.
The threads used in the program jumped between logical cores frequently, even when running the program for long periods of time with large particle counts. It is not clear how much of a performance hit these transfers have caused on the application. Changing OpenMP scheduling parameters did not seem to produce a noticeable difference to this.

\subsection{Speedup}
Shown in Figure \ref{fig:CpuSpeedupGraph} and Figure \ref{fig:CpuSpeedup}
for above 2048 particles, speedup is roughly ranked by threads count, with 8 threads producing the highest speedup. A thread count of 4 produced a fluctuating speedup, coming close to beating 6 threads at times. This could be due to the CPU having 4 physical cores, which are hyper-threaded to 8 logical cores.

\subsection{Efficiency}
Shown in Figure \ref{fig:CpuEfficiencyGraph} and Figure \ref{fig:CpuEfficiency}.
While running with 2 threads produced the least amount of speedup, it was far ahead of any other thread count in terms of efficiency. This was true for all but the largest particle count of 524288, where both 3 and 4 threads were more efficient and than 2. This is most probably an artifact of the CPU/Operating System thread scheduling when put under heavy usage.

\section{GPU implementation}
The GPU version of the application is written as a DirectX12 Compute Shader, the output of the computation was passed to a geometry shader which turned the simulation body data into triangles which could then rendered. This method keeps all the work on the GPU with the CPU only used for synchronization. 
As the velocity data is already on the GPU and accessible by the renderer, it was used to colour the particles, faster particles are rendered with a brighter colour.

\figuremacroW
{gpusim2}
{Gpu Simulation}
{65536 Particles}
{1.0}

\subsection{GPU Computation time}
The hardware used was a Nvidia GTX 980, which has 16 Multiprocessors, with each with 128 Streaming processors (2048 total cores).\\
The GPU is well suited for this type of application, as the results show, the GPU is between 15\% and 160\% faster than a cpu running with 8 threads. The GPU kept below 16ms (required for 60fps) computation time for up to 32768 particles (The CPU could only do 2048, using 8 threads).

\subsection{Work Group Size}
Work is dispatched to the GPU in 3 Dimensional groups of kernels. 
A group of kernels should theoretically all run at the same time, as N-Body requires no intercommunication or synchronizing between kernels, the group size could be set to any size supported by the hardware.
This report measures the effect of varying the work group size (Figure \ref{fig:GpuTmePerPArticleGraph} and Figure \ref{fig:GpuTmePerPArticle}).
\\ 
Using DirectX 12 a group dimension cannot exceed 65535, hence the missing value for (16,524288) in the results.
\\
Analyzing the data, it seems that a group size of 32 consistently provides the quickest computation time. This could be due to 32 also being the "Warp Size" of the hardware, which is the amount of instructions that are issued at once from a multiprocessor

\section{Future work}

\subsection{Optimization algorithms}
There are existing algorithms to reduce the problem size by using varied forms of approximation.

By grouping the Bodies into discreet areas (normally a tree structure), groups of bodies can be averaged to one central point for an area. When looping through each body, calculations may only use the averaged position of a collection of bodies, if the area they reside in is far enough away.
This approach theoretically reduced the complexity from $O(n^2)$ to $O(n\,log \,n)$
\cite{1986Natur.324..446B}

Implementing the method for creating and traversing a tree of bodies is relatively straightforward on the CPU and can have large performance gains. Implementing the same thing on a GPU would be a harder task \cite{goodcite}. Parallelizing the tree creation stage would require frequent synchronization, as the dimensions and state of the tree change for every body that is added to it.

\section{Conclusions}
This report evaluated the performance of two similar implantations of an N-Body particle simulation. The parallel performance of a CPU was compared to that of the GPU and it was found that the GPU is significantly faster at this task. Using threads on the CPU was found to be worthwhile, but the upper limit of performance was still significantly behind that of the GPU.
N-Body simulations are very easily converted from sequential to parallel, due to there being no dependencies between each bodies calculations, therefore this type if simulation provides a good benchmark of total available compute resources.

\bibliographystyle{acmsiggraph}
\bibliography{report}

\clearpage
\section{Appendix}

\begin{table}[b]
	{
		\centering
		\resizebox{1.0\textwidth}{!}{
			\begin{minipage}{\textwidth}
				\centering
				\begin{tabular}{p{1.7cm}|cccccccccc}
					Test Name & 1024 & 2048 & 4096 & 8192 & 16384 & 32768 & 65536 & 131072 & 262144 & 524288\\ \hline \\
					GTX980 512 & 0.233 & 0.321 & 0.652 & 1.145 & 1.974 & 9.219 & 36.789 & 135.558 & 500.509 & 2100.374\\
					GTX980 256 & 0.222 & 0.307 & 0.612 & 1.065 & 1.937 & 9.162 & 34.392 & 134.494 & 489.240 & 2051.084\\
					GTX980 128 & 0.132 & 0.194 & 0.296 & 0.716 & 1.891 & 8.721 & 53.158 & 120.636 & 471.306 & 1902.573\\
					GTX980 64 & 0.132 & 0.172 & 0.277 & 0.645 & 1.862 & 7.259 & 31.745 & 118.255 & 451.687 & 1901.846\\
					GTX980 32 & 0.110 & 0.157 & 0.289 & 0.640 & 1.833 & 7.255 & 28.108 & 116.337 & 451.348 & 1899.133\\
					GTX980 16 & 0.150 & 0.224 & 0.464 & 1.468 & 3.561 & 13.522 & 63.056 & 222.645 & 917.328 & 0.000\\
					\hline\\
					CPU 1 core &  4.898 & 19.137 & 74.248 & 296.022 & 1185.570 & 4798.358 & 19412.055 & 76464.740 & 314798.033 & 1227484.000\\
					CPU 2 cores &  2.510 & 9.825 & 39.221 & 155.133 & 613.820 & 2451.838 & 9838.845 & 40324.536 & 167580.294 & 797311.542\\
					CPU 3 cores &  2.180 & 9.193 & 31.041 & 119.769 & 455.580 & 1821.591 & 7185.361 & 27788.974 & 115910.695 & 496237.950\\
					CPU 4 cores &  1.674 & 7.044 & 22.304 & 98.299 & 410.325 & 1597.991 & 5535.962 & 21020.147 & 95596.342 & 377586.191\\
					CPU 5 cores &  2.018 & 6.930 & 25.998 & 89.877 & 375.376 & 1455.549 & 5624.133 & 22419.623 & 92536.925 & 366961.358\\
					CPU 6 cores &  1.547 & 6.081 & 23.429 & 89.602 & 350.761 & 1350.470 & 5229.090 & 20683.325 & 84305.466 & 350425.495\\
					CPU 7 cores &  1.538 & 5.044 & 20.425 & 82.377 & 322.066 & 1282.543 & 4918.743 & 19273.623 & 78993.126 & 325729.737\\
					CPU 8 cores &  1.674 & 5.106 & 18.330 & 72.642 & 285.565 & 1178.308 & 4643.889 & 18259.532 & 72826.053 & 302954.399\\
					\hline
				\end{tabular}
				
				\caption[Table caption text]{Results of all tests, Times in Milliseconds\\
					}
				\label{table:results}
			\end{minipage} }
		}
	\end{table}


\figuremacroF
{cpuperf1}
{Cpu Compute time for less than 4096 Particles}
{Time Measured in Nanoseconds}
{1.0}

\figuremacroF
{cpuperf2}
{Cpu Compute time for greater than 65536 Particles}
{Time Measured in Nanoseconds}
{1.0}


\figuremacroF
{CpuSpeedupGraph}
{Cpu Speedup}
{}
{1.0}

\figuremacroF
{CpuEfficiencyGraph}
{Cpu Efficiency}
{Speedup measured against core count}
{1.0}


\figuremacroF
{CpuSpeedup}
{Cpu Speedup}
{Colour coded scale for visual clarity}
{1.0}

\figuremacroF
{CpuEfficiency}
{Cpu Efficiency}
{Speedup measured against core count}
{1.0}

\figuremacroF
{omp1_4096_occu}
{Cpu Occupancy}
{1 Core, 4096 Particles}
{1.0}

\figuremacroF
{omp4_4096_occu}
{Cpu Occupancy}
{4 Cores, 4096 Particles}
{1.0}

\figuremacroF
{omp8_4096_occu}
{Cpu Occupancy}
{8 Cores, 4096 Particles}
{1.0}

\figuremacroF
{omp4_65536_util}
{Cpu Utilization}
{4 Cores, 65536 Particles}
{1.0}

\figuremacroF
{omp8_65536_util}
{Cpu Utilization}
{8 Cores, 65536 Particles}
{1.0}


\figuremacroF
{GpuTmePerPArticleGraph}
{GpuTme Per Particle}
{Varying Workgroup size}
{1.0}

\figuremacroF
{GpuTmePerPArticle}
{GpuTme Per Particle}
{Varying Workgroup size}
{1.0}



\end{document}

