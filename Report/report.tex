
\documentclass[conference]{acmsiggraph}

\usepackage{graphicx}
\graphicspath{{./images/}}

\newcommand{\figuremacroW}[4]{
	\begin{figure}[h] %[htbp]
		\centering
		\includegraphics[width=#4\columnwidth]{#1}
		\caption[#2]{\textbf{#2} - #3}
		\label{fig:#1}
	\end{figure}
}

\newcommand{\figuremacroF}[4]{
	\begin{figure*}[t] % [htbp]
		\centering
		\includegraphics[width=#4\textwidth]{#1}
		\caption[#2]{\textbf{#2} - #3}
		\label{fig:#1}
	\end{figure*}
}

\usepackage{afterpage}
\usepackage{xcolor}
\definecolor{lbcolor}{rgb}{0.98,0.98,0.98}
\usepackage{listings}

\lstset{
	escapeinside={/*@}{@*/},
	language=C++,
	%basicstyle=\small\sffamily,
	%basicstyle=\small\sffamily,	
	basicstyle=\fontsize{8.5}{12}\selectfont,
	%basicstyle=\small\ttfamily,
	%basicstyle=\scriptsize, % \footnotesize,
	%basicstyle=\footnotesize,
	%keywordstyle=\color{blue}\bfseries,
	%basicstyle= \listingsfont,
	numbers=left,
	numbersep=2pt,    
	xleftmargin=2pt,
	%numberstyle=\tiny,
	frame=tb,
	%frame=single,
	columns=fullflexible,
	showstringspaces=false,
	tabsize=4,
	keepspaces=true,
	showtabs=false,
	showspaces=false,
	%showstringspaces=true
	backgroundcolor=\color{lbcolor},
	morekeywords={inline,public,class,private,protected,struct},
	captionpos=t,
	lineskip=-0.4em,
	aboveskip=10pt,
	%belowskip=50pt,
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941}
}

\usepackage{lipsum}

\title{Optimising the LINPACK 1000 using Parallelization}

\author{Sam Serrels\\\ 40082367@napier.ac.uk \\
Edinburgh Napier University\\
Concurrent and Parallel Systems (SET10108)}
\pdfauthor{Sam Serrels}

\keywords{Vehicle Routing, Clarke Wright}

\begin{document}

\maketitle

\section{Introduction}

\paragraph{The LINPACK benchmark}
The Linpack algorithm is a popular benchmark in the high performance computing field as it is uses as a floating point performance measure for ranking supercomputers.
The Benchmark solves a large system of linear equations using LU decomposition and is typical of many matrix-based scientific computations.
Linpack started as a Fortran maths processing application, the code for solving linear equations was extracted from the original program and turned into a benchmark.  
\\
The Linpack1000 algorithm first generates a random 1000x1000 element matrix, A, and 1000 element vector, b. The elements in A and b are all double precision floating point numbers. 
Processing then takes place to find the solution, a 1000 element vector, x, such that Ax = b.
\\
This report documents the process of analysing a specific sequential Linpack implementation and then converting it to a parallel task.

\paragraph{Related Work}
As Linpack is used to benchmark large scale multiprocessor supercomputers, there are many parallel versions available. 
The main difference between implementations is the Gaussian elimination stage, for which there are many algorithms available, some of which can split the task up into separate easily parallelizable chunks of logic. For the scope of this project, changes to the algorithm were avoided wherever possible to keep a fair comparison to the original sequential code.

\paragraph{High-Performance Linpack}
The most commonly used implantation of Linpack is the HPL implementation, written by the Innovative Computing Laboratory at the University of Tennessee. 
HPL is an open-source project that aims to provide a toolbox for configuring, optimising, and running the benchmark over a network. 
It contains many version of the algorithm with plenty of configurable options and for tuning performance to a specific system.
\\
The HPL project was used as a rough guide to how the reference algorithm could be modified for this project, however most of the optimisation were beyond the scope of this project.

\paragraph{Project Scope}
A quick optimisation would be to drop the precision of the algorithm from double, to single precision floating point values.
Another method would be to swap the original Gaussian elimination algorithm for a different mathematical approach that would lend itself to parallel processing better.
This project aimed to see how much the original Linpack code can be optimised with parallelization, without changing the core logic of the algorithm or data output so these routes for optimisation were ruled out.

\paragraph{OpenMP}

\paragraph{SIMD}

\section{Linpack Gaussian elimination}
The dgefa and dgesl subroutines are then used to find a 1000 element vector, x, such that Ax = b.
The dgefa subroutine performs LU decomposition by Gaussian elimination with partial pivoting on A. A brief
dgefa modifies A and returns a vector, ipvt, containing the pivot
indices. 

The dgesl subroutine then solves the simplified LU version of the original system. 
The time taken to completethe dgefa and dgesl subroutines is measured.
The remainder of the algorithmserves to estimate the normalized residual error in the result vector x. 


\paragraph{Initial analysis}
The generated routes were exported as a .csv data file, and a visualisation of the routes were also generated and saved as a .svg vector image format. These images are included in the report and can bee seen in the results section.

\figuremacroW
{linpackmat}
{Sequential Algorithm with 50 Customers}
{}
{0.75}


\paragraph{The Pivot loop}
A basic solution in which a single truck was allocated to every customer was created, forming the most ineffective solution possible. This was used as the baseline set of data that the Clarke-Wright solutions were compared against. The solution cost and total number of routes for each algorithm were compared to each other and to the baseline numbers.

\paragraph{The Collumn loop}
Test were carried to measure the relative computation time for each algorithm with an increasing set of customers. The tests were carried out fifty times and the results averaged to mitigate external factors.\\
The program was executed on a linux based server, the Java Virtual machine was allocated an initial memory size of 2GB to avoid heap increase slowdowns and the JVM was also running in server mode. These conditions should result in the best possible testing environment in terms of  repeatable and consistent results.

\paragraph{Daxpy}
A basic solution in which a single truck was allocated to every customer was created, forming the most ineffective solution possible. This was used as the baseline set of data that the Clarke-Wright solutions were compared against. The solution cost and total number of routes for each algorithm were compared to each other and to the baseline numbers.

\section{Optimisation Method}

\paragraph{Code Simplification}
Each solution produced by the algorithms was tested by looping through the routes, checking that each customer was visited and that the correct quantity was delivered. The total quantity of deliveries for each route was also calculated, assuring that no route was over the capacity of the truck.

\paragraph{Parallelised Daxpy}
Each solution produced by the algorithms was tested by looping through the routes, checking that each customer was visited and that the correct quantity was delivered. The total quantity of deliveries for each route was also calculated, assuring that no route was over the capacity of the truck.

\paragraph{SIMD Daxpy}
Each solution produced by the algorithms was tested by looping through the routes, checking that each customer was visited and that the correct quantity was delivered. The total quantity of deliveries for each route was also calculated, assuring that no route was over the capacity of the truck.

\paragraph{MDaxpy}
Each solution produced by the algorithms was tested by looping through the routes, checking that each customer was visited and that the correct quantity was delivered. The total quantity of deliveries for each route was also calculated, assuring that no route was over the capacity of the truck.

\afterpage{\clearpage}


\clearpage
\begin{table}[b]
{
\resizebox{1.0\textwidth}{!}{
\begin{minipage}{\textwidth}
\centering
    \begin{tabular}{ccccccccc}
						    &	Allocate&	Create Input &	gaussian&	&	&	Total&	Total &	Speedup\\ 
		Name				&	Memory (ms)&	Numbers (ms)&	eliminate (ms)&	Solve (ms)&	Validate (ms)&	Time (ms)&	Speedup&	(With Simd)$^{*}$\\ \hline\hline \\
		Threads: 1 No Simd	&	0.48	&	5.22	&	157.27	&	0.69	&	5.27	&	168.93	&	0\%	&	0\%     \\
		Threads: 1 Simd128	&	0.45	&	5.00	&	152.68	&	0.67	&	5.06	&	163.85	&	3\%	&	0\%     \\
		Threads: 1 Simd256	&	0.44	&	5.06	&	142.04	&	0.67	&	5.14	&	153.34	&	9\%	&	0\%     \\
		Threads: 2 No Simd	&	0.42	&	5.48	&	78.76	&	0.68	&	5.52	&	90.86	&	46\%	&	46\%\\
		Threads: 2 Simd128	&	0.46	&	5.36	&	85.35	&	0.66	&	5.49	&	97.32	&	42\%	&	41\%\\
		Threads: 2 Simd256	&	0.45	&	4.82	&	72.86	&	0.63	&	4.83	&	83.59	&	51\%	&	45\%\\
		Threads: 3 No Simd	&	0.45	&	5.37	&	63.49	&	0.68	&	5.46	&	75.44	&	55\%	&	55\%\\
		Threads: 3 Simd128	&	0.46	&	5.52	&	60.00	&	0.66	&	5.64	&	72.28	&	57\%	&	56\%\\
		Threads: 3 Simd256	&	0.46	&	5.74	&	52.51	&	0.58	&	5.85	&	65.14	&	61\%	&	58\%\\
		Threads: 4 No Simd	&	0.44	&	5.28	&	42.67	&	0.62	&	5.33	&	54.34	&	68\%	&	68\%\\
		Threads: 4 Simd128	&	0.43	&	5.72	&	41.51	&	0.54	&	5.78	&	53.97	&	68\%	&	67\%\\
		Threads: 4 Simd256	&	0.48	&	5.80	&	39.61	&	0.55	&	5.91	&	52.36	&	69\%	&	66\%\\
		Threads: 5 No Simd	&	0.46	&	5.41	&	50.89	&	0.67	&	5.50	&	62.93	&	63\%	&	63\%\\
		Threads: 5 Simd128	&	0.45	&	5.20	&	49.49	&	0.67	&	5.24	&	61.05	&	64\%	&	63\%\\
		Threads: 5 Simd256	&	0.46	&	5.11	&	46.46	&	0.60	&	5.24	&	57.86	&	66\%	&	62\%\\
		Threads: 6 No Simd	&	0.50	&	5.80	&	44.44	&	0.75	&	5.93	&	57.41	&	66\%	&	66\%\\
		Threads: 6 Simd128	&	0.48	&	5.53	&	43.04	&	0.69	&	5.66	&	55.41	&	67\%	&	66\%\\
		Threads: 6 Simd256	&	0.49	&	5.55	&	41.63	&	0.65	&	5.72	&	54.04	&	68\%	&	65\%\\
		Threads: 7 No Simd	&	0.53	&	6.00	&	40.21	&	0.75	&	6.18	&	53.66	&	68\%	&	68\%\\
		Threads: 7 Simd128	&	0.52	&	5.40	&	40.02	&	0.73	&	5.55	&	52.22	&	69\%	&	68\%\\
		Threads: 7 Simd256	&	0.55	&	5.58	&	35.69	&	0.68	&	5.77	&	48.28	&	71\%	&	69\%\\
		Threads: 8 No Simd	&	0.51	&	5.01	&	43.25	&	0.80	&	5.17	&	54.75	&	68\%	&	68\%\\
		Threads: 8 Simd128	&	0.53	&	5.48	&	43.44	&	0.78	&	5.73	&	55.96	&	67\%	&	66\%\\
		Threads: 8 Simd256	&	0.51	&	5.47	&	38.64	&	0.71	&	5.72	&	51.05	&	70\%	&	67\%\\
		\hline
    \end{tabular}
   
    \caption[Table caption text]{Results of all tests\\
    	*Speed-up with Simd, compares times against the	Simd equivalent 1 Thread run}
    \label{table:name}
    \end{minipage} }
}
\end{table}

\figuremacroF
{graph2}
{Total Speed-up percentage, for each number of threads}
{}
{1.0}
\figuremacroF
{graph1}
{Total Speed-up (Relative to baseline Simd) percentage, for each number of threads}
{}
{1.0}
\clearpage

\figuremacroF
{t1i8simd256conc}
{Single Threaded, 6 runs, simd256 Daxpy}
{Overall system CPU utilisation}
{1.0}

\figuremacroF
{t1i8simd256cond}
{Single Threaded, 6 runs, simd256 Daxpy}
{Thread to CPU Core allocation}
{1.0}

\figuremacroF
{t1i8simd256conp}
{Single Threaded, 6 runs, simd256 Daxpy}
{Thread Status}
{1.0}

\figuremacroF
{t4i8simd256conc}
{4 Threads, 6 runs, simd256 Daxpy}
{Overall system CPU utilisation}
{1.0}

\figuremacroF
{t4i8simd256cond}
{4 Threads, 6 runs, simd256 Daxpy}
{Thread to CPU Core allocation}
{1.0}

\figuremacroF
{t4i8simd256conp}
{4 Threads, 6 runs, simd256 Daxpy}
{Thread Status}
{1.0}

\figuremacroF
{t8i8simd256conc}
{8 Threads, 6 runs, simd256 Daxpy}
{Overall system CPU utilisation}
{1.0}

\figuremacroF
{t8i8simd256cond}
{8 Threads, 6 runs, simd256 Daxpy}
{Thread to CPU Core allocation}
{1.0}

\figuremacroF
{t8i8simd256conp}
{8 Threads, 6 runs, simd256 Daxpy}
{Thread Status}
{1.0}


\section{Conclusions}
\paragraph{Computation time}
Both Implementations of the Clarke-Wright algorithms produced expected results, with the parallel version producing larger and fewer routes. As for the time taken to calculate, the performance is roughly equal.
The discrepancies shown in Figure \ref{fig:chart1} when the amount of customers increases beyond 800 is possibly due to optimisations carried out by the Java virtual machine. The total operations carried out is roughly the same for each algorithm, however the arrays are accessed and modified at different times, this is a possible cause for the difference in processing time.

\paragraph{Solution Quality}
The Parallel solution produced a large saving of up to a 600\% increase against the baseline cost, shown in Figure \ref{fig:chart2}. The Sequential solution produced a constant saving of around 200\%. These results are also shown in Figure \ref{fig:chart3}, showing the number of routes.

\paragraph{Edge Cases}
It is possible that a customer can be left out of all routes due to capacity constraints; this is checked for at the end of the calculation. If a customer is left over, it is seen if it would be possible to add it to any existing route and then if it would be more efficient than sending out a new truck. This can be seen in Figure \ref{fig:rand00020cwpsn}, the customer in the top left falls in to this edge case category.

\paragraph{Conclusion}
The implementation written for this report successfully computes optimised and usable data, the processing cost increases in a quadratic relation to the size of data. The specific implementation could be optimised to produce quicker results. One possible optimisation route could be a custom sort method, as profiling reported that 40\% of the processing time is taken by the initial sort of the customer pairs.\\
Overall this report produced repeatable and meaningful data, and can be seen as a successful investigation into the Clark-Wright Algorithm.
\bibliographystyle{acmsiggraph}
\bibliography{report}
\clearpage

\section{Appendix: Code}
\subsection{ClarkeWright.java}
%\lstinputlisting[language=Java]{../src/ClarkeWright.java}

\vfill

\subsection{VRSolution.java}
Lines 20 to 28
%\lstinputlisting[language=Java, firstline=20, lastline=27]{../src/VRSolution.java}

\subsection{Experiment.java}
%\lstinputlisting[language=Java]{../src/Experiment.java}

\end{document}

